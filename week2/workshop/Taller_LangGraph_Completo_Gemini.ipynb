{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3008410-intelligent-systems/blob/main/week2/workshop/Taller_LangGraph_Completo_Gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2ef653",
      "metadata": {
        "id": "4e2ef653"
      },
      "source": [
        "# Taller: LangGraph para Chatbots Inteligentes (Gemini)\n",
        "**Objetivo:** practicar desde lo básico hasta memoria y checkpointing (incluye time travel y streaming).  \n",
        "**Requisitos:** tener `GOOGLE_API_KEY` en Colab Secrets.\n",
        "\n",
        "Estructura del taller:\n",
        "1) Tema + ejemplo corto  \n",
        "2) “Tu turno”: modifica/crea tu propio código  \n",
        "3) Preguntas teóricas y “¿qué pasa si…?”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "062bb1ea",
      "metadata": {
        "id": "062bb1ea"
      },
      "outputs": [],
      "source": [
        "%pip install -U langgraph langchain-groq pydantic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b54bc9e",
      "metadata": {
        "id": "6b54bc9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.getenv(\"GROQ_API_KEY\"):\n",
        "    try:\n",
        "        os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"GROQ_API_KEY:\", \"✅\" if os.getenv(\"GROQ_API_KEY\") else \"⚠️\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tc0QpD-EVwne",
      "metadata": {
        "id": "tc0QpD-EVwne"
      },
      "outputs": [],
      "source": [
        "\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.2)\n",
        "print(\"✅ LLM listo:\", llm.model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa7c8f55",
      "metadata": {
        "id": "aa7c8f55"
      },
      "source": [
        "## 1) Estados, Nodos y Aristas (los 3 pilares)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02d0c1f3",
      "metadata": {
        "id": "02d0c1f3"
      },
      "source": [
        "### Ejemplo 1: grafo lineal (START → nodo → END)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d30ff05",
      "metadata": {
        "id": "0d30ff05"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "class S1(TypedDict):\n",
        "    text: str\n",
        "    upper: str\n",
        "\n",
        "def to_upper(state: S1):\n",
        "    return {\"upper\": state[\"text\"].upper()}\n",
        "\n",
        "g = StateGraph(S1)\n",
        "g.add_node(\"to_upper\", to_upper)\n",
        "g.add_edge(START, \"to_upper\")\n",
        "g.add_edge(\"to_upper\", END)\n",
        "app1 = g.compile()\n",
        "\n",
        "app1.invoke({\"text\": \"hola langgraph\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35941a12",
      "metadata": {
        "id": "35941a12"
      },
      "source": [
        "### Tu turno\n",
        "1) Crea un nodo `to_lower` que convierta a minúsculas.  \n",
        "2) Haz un grafo que primero haga `to_upper` y luego `to_lower` (para ver el flujo)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d95bc0e",
      "metadata": {
        "id": "9d95bc0e"
      },
      "source": [
        "### Preguntas (¿qué pasa si...?)\n",
        "- ¿Qué pasa si tu estado no tiene la key que el nodo espera?\n",
        "- ¿Qué pasa si olvidas conectar un nodo a `END`?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf029e41",
      "metadata": {
        "id": "cf029e41"
      },
      "source": [
        "## 2) Aristas condicionales (routing)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93f27a81",
      "metadata": {
        "id": "93f27a81"
      },
      "source": [
        "### Ejemplo 2: decidir ruta según el input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4954d9c",
      "metadata": {
        "id": "f4954d9c"
      },
      "outputs": [],
      "source": [
        "from typing import Literal\n",
        "\n",
        "class S2(TypedDict):\n",
        "    text: str\n",
        "    kind: str\n",
        "    out: str\n",
        "\n",
        "def classify(state: S2):\n",
        "    t = state[\"text\"].lower()\n",
        "    kind = \"pregunta\" if \"?\" in t else \"afirmacion\"\n",
        "    return {\"kind\": kind}\n",
        "\n",
        "def responder_pregunta(state: S2):\n",
        "    resp = llm.invoke(f\"Responde breve: {state['text']}\")\n",
        "    return {\"out\": resp.content}\n",
        "\n",
        "def resumir_afirmacion(state: S2):\n",
        "    resp = llm.invoke(f\"Resume en 1 frase: {state['text']}\")\n",
        "    return {\"out\": resp.content}\n",
        "\n",
        "def route(state: S2) -> Literal[\"responder_pregunta\", \"resumir_afirmacion\"]:\n",
        "    return \"responder_pregunta\" if state[\"kind\"] == \"pregunta\" else \"resumir_afirmacion\"\n",
        "\n",
        "g2 = StateGraph(S2)\n",
        "g2.add_node(\"classify\", classify)\n",
        "g2.add_node(\"responder_pregunta\", responder_pregunta)\n",
        "g2.add_node(\"resumir_afirmacion\", resumir_afirmacion)\n",
        "\n",
        "g2.add_edge(START, \"classify\")\n",
        "g2.add_conditional_edges(\"classify\", route)\n",
        "g2.add_edge(\"responder_pregunta\", END)\n",
        "g2.add_edge(\"resumir_afirmacion\", END)\n",
        "\n",
        "app2 = g2.compile()\n",
        "\n",
        "print(app2.invoke({\"text\":\"¿Qué es LangGraph?\"})[\"out\"])\n",
        "print(app2.invoke({\"text\":\"LangGraph sirve para orquestar flujos con grafos.\"})[\"out\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536d5f92",
      "metadata": {
        "id": "536d5f92"
      },
      "source": [
        "### Tu turno\n",
        "Cambia la regla de routing:\n",
        "- Si el texto contiene “pasos” → usa un nodo `dar_pasos` (lista 1-3 items)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2016d1d7",
      "metadata": {
        "id": "2016d1d7"
      },
      "source": [
        "### Preguntas\n",
        "- ¿Qué pasa si tu función `route` devuelve un nombre de nodo que no existe?\n",
        "- ¿Qué pasa si ambos caminos llevan a más nodos y no solo a END?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "586b01a8",
      "metadata": {
        "id": "586b01a8"
      },
      "source": [
        "## 3) Gestión de conversación: historial, truncamiento y resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45f3044",
      "metadata": {
        "id": "c45f3044"
      },
      "source": [
        "### Ejemplo 3: MessagesState + add_messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f479efe4",
      "metadata": {
        "id": "f479efe4"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import MessagesState, add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# MessagesState ya trae la key \"messages\" con reducer append-only\n",
        "def chatbot_node(state: MessagesState):\n",
        "    resp = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [resp]}\n",
        "\n",
        "g3 = StateGraph(MessagesState)\n",
        "g3.add_node(\"chat\", chatbot_node)\n",
        "g3.add_edge(START, \"chat\")\n",
        "g3.add_edge(\"chat\", END)\n",
        "app3 = g3.compile()\n",
        "\n",
        "state0 = {\"messages\": [HumanMessage(content=\"Hola, ¿quién eres?\")]}\n",
        "out = app3.invoke(state0)\n",
        "out[\"messages\"][-1].content"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd7570a",
      "metadata": {
        "id": "0bd7570a"
      },
      "source": [
        "### Tu turno\n",
        "- Haz 2 turnos: agrega el mensaje del usuario y vuelve a invocar.\n",
        "- Observa cómo crece la lista `messages`.\n",
        "- Implementa una paso para quitar mensajes\n",
        "- Implementa una paso para resumir mensajes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6884d49e",
      "metadata": {
        "id": "6884d49e"
      },
      "source": [
        "### Preguntas\n",
        "- ¿Por qué guardar todo el historial puede ser costoso?\n",
        "- ¿Qué ventajas tiene “resumir” vs “borrar” mensajes?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}