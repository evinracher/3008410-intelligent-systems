{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evinracher/3008410-intelligent-systems/blob/main/week2/exercise1/Fine_Tunning_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "61b78358-4060-48f8-9060-200e146ae636",
      "metadata": {
        "id": "61b78358-4060-48f8-9060-200e146ae636",
        "outputId": "6c4bc452-f94a-49e3-9dcf-e4643edcd76d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TF_USE_LEGACY_KERAS=1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.19.0 requires tensorflow<2.20,>=2.19.0, but you have tensorflow 2.20.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.12.0 requires tensorflow==2.19.0, but you have tensorflow 2.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%env TF_USE_LEGACY_KERAS=1\n",
        "\n",
        "!pip -q uninstall -y keras transformers tensorflow tf-keras safetensors\n",
        "!pip -q install \\\n",
        "  \"tensorflow==2.20.0\" \\\n",
        "  \"tf-keras\" \\\n",
        "  \"transformers==4.49.0\" \\\n",
        "  \"safetensors==0.4.5\" \\\n",
        "  \"datasets==2.20.0\" \\\n",
        "  \"evaluate==0.4.2\" \\\n",
        "  \"accelerate==0.34.2\" \\\n",
        "  \"sentencepiece==0.2.0\" rouge_score nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f19ce230",
      "metadata": {
        "id": "f19ce230"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM, create_optimizer, AdamWeightDecay, pipeline\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e7c7bb6",
      "metadata": {
        "id": "4e7c7bb6",
        "outputId": "f8699ad5-9e6f-450c-ce80-d918153d2f5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79206908",
      "metadata": {
        "id": "79206908",
        "outputId": "6427c2ba-400d-46d7-bc04-d5ec68389a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C:\\Users\\John\\Downloads\\Universidad\\\\AI Generativa Course\\Exercises\\Exercise2\\eng_small.csv\n",
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['engl', 'spa'],\n",
            "        num_rows: 14074\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['engl', 'spa'],\n",
            "        num_rows: 6033\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "folder_path = r\"C:\\Users\\John\\Downloads\\Universidad\\\\AI Generativa Course\\Exercises\\Exercise2\"\n",
        "dataset_name = \"eng_small.csv\"\n",
        "path = os.path.join(folder_path, dataset_name)\n",
        "print(path)\n",
        "data = Dataset.from_csv(path, encoding='utf-8')\n",
        "data = data.train_test_split(test_size=0.1)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c22f593b-909d-48f5-8c13-890fe063162f",
      "metadata": {
        "id": "c22f593b-909d-48f5-8c13-890fe063162f",
        "outputId": "5ab2c53b-1c12-4ebb-f84b-d761b9972caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\John\\miniconda312\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")  #google/flan-t5-small\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\") #google/flan-t5-small\n",
        "#model = export_and_get_onnx_model('t5-small')\n",
        "\n",
        "prefix = \"translate: \"\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b302007e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "90b33df218dc410287c8802bd267bb5c",
            "8f271bbbd8c5455cbc31c904fdafa662"
          ]
        },
        "id": "b302007e",
        "outputId": "2b45aeb6-c8c6-47df-b44e-aa6c4201a601"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90b33df218dc410287c8802bd267bb5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/14074 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f271bbbd8c5455cbc31c904fdafa662",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6033 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_data = data.map(preprocess_function, batched=True, remove_columns=[\"engl\", \"spa\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cd75ab",
      "metadata": {
        "id": "f3cd75ab"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-4, weight_decay_rate=0.01) #2e-5 was before wd was 1e-2, Typically, 1e-4 and 3e-4 work well for most problems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "076839d6",
      "metadata": {
        "id": "076839d6"
      },
      "outputs": [],
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541fe8a7",
      "metadata": {
        "id": "541fe8a7"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6",
      "metadata": {
        "id": "b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6",
        "outputId": "a365c25e-bfc9-436e-992f-5026736fb417"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "WARNING:tensorflow:From C:\\Users\\John\\miniconda312\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "879/879 [==============================] - 770s 845ms/step - loss: 2.2588 - val_loss: 1.5672\n",
            "Epoch 2/5\n",
            "879/879 [==============================] - 721s 820ms/step - loss: 1.6173 - val_loss: 1.2618\n",
            "Epoch 3/5\n",
            "879/879 [==============================] - 700s 796ms/step - loss: 1.3303 - val_loss: 1.1035\n",
            "Epoch 4/5\n",
            "879/879 [==============================] - 711s 809ms/step - loss: 1.1491 - val_loss: 1.0097\n",
            "Epoch 5/5\n",
            "879/879 [==============================] - 797s 906ms/step - loss: 1.0180 - val_loss: 0.9469\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x20739814fe0>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs, callbacks=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8fa8e30",
      "metadata": {
        "id": "e8fa8e30"
      },
      "outputs": [],
      "source": [
        "# Guarda el modelo entrenado\n",
        "folder_path = 'model'\n",
        "model_name = \"NMT-epocs-\" + str(epochs)\n",
        "path = os.path.join(folder_path, model_name + \".h5\")\n",
        "model.save_pretrained(path)\n",
        "del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f69e21f",
      "metadata": {
        "id": "6f69e21f",
        "outputId": "9c72bb61-b781-43ec-e25a-1e8b4ac0ba8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at model\\NMT-epocs-5.h5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "C:\\Users\\John\\miniconda312\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1156: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n",
            "  warnings.warn(\n",
            "Device set to use 0\n"
          ]
        }
      ],
      "source": [
        "#Para inferir desde aqu√≠.\n",
        "model_name = \"NMT-epocs-\" + str(epochs)\n",
        "path = os.path.join(folder_path, model_name + \".h5\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(path, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "summarizer = pipeline(\"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c3925f",
      "metadata": {
        "id": "d7c3925f",
        "outputId": "26bef9bb-a976-4c2a-ad9a-6a7e7b228b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'translation_text': 'Es buen momento de ir a la boca.'}]\n",
            "time: 3.91 seconds\n"
          ]
        }
      ],
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "text = \"translate: it's summer it is nice to go to the beach\"\n",
        "print(summarizer(text, min_length=4, max_length=100))\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"time: {round(elapsed,2)} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef20d606-c6a8-403c-a028-fe622b431589",
      "metadata": {
        "id": "ef20d606-c6a8-403c-a028-fe622b431589"
      },
      "source": [
        "# Exercise\n",
        "\n",
        "- This time use the larger dataset (eng.csv), use the same sentence and see the results.\n",
        "- Modify the code to graph and report the Rouge metric (*)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4431bc7-718c-485a-947e-aa54281b614f",
      "metadata": {
        "id": "e4431bc7-718c-485a-947e-aa54281b614f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5244d87-da20-4ff6-98db-b00cd2b665ac",
      "metadata": {
        "id": "d5244d87-da20-4ff6-98db-b00cd2b665ac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c34e795d-21d0-42c2-8799-5ccaec0e4413",
      "metadata": {
        "id": "c34e795d-21d0-42c2-8799-5ccaec0e4413"
      },
      "source": [
        "If you want to try other tokenizers, see next"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f6c3bcf-10bc-4d3b-8c10-876d4754aaba",
      "metadata": {
        "id": "0f6c3bcf-10bc-4d3b-8c10-876d4754aaba"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d7a3d57-1e6d-4c42-8330-87e4c724853e",
      "metadata": {
        "id": "2d7a3d57-1e6d-4c42-8330-87e4c724853e"
      },
      "outputs": [],
      "source": [
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6588008-4c89-4693-8b88-1b63663c34cd",
      "metadata": {
        "id": "b6588008-4c89-4693-8b88-1b63663c34cd"
      },
      "outputs": [],
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfc1cd5-41cd-44d2-b8b6-556475ad9eb7",
      "metadata": {
        "id": "5bfc1cd5-41cd-44d2-b8b6-556475ad9eb7"
      },
      "outputs": [],
      "source": [
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
        "mymodel = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33d6e30-4422-4fa4-8f68-0ec1aa54001c",
      "metadata": {
        "id": "c33d6e30-4422-4fa4-8f68-0ec1aa54001c"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "- Change summarize by translate\n",
        "- Graph the Rouge metric\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "11cfaa01c2d489d556e66fcaf9e384c983411f23416837fdb98f5bb9e5af2b30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}